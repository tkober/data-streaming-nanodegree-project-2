:: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-44fb42eb-2eac-450f-810a-ab97739a323f;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central
	found org.apache.kafka#kafka-clients;2.6.0 in central
	found com.github.luben#zstd-jni;1.4.8-1 in central
	found org.lz4#lz4-java;1.7.1 in central
	found org.xerial.snappy#snappy-java;1.1.8.2 in central
	found org.slf4j#slf4j-api;1.7.30 in central
	found org.spark-project.spark#unused;1.0.0 in central
	found org.apache.commons#commons-pool2;2.6.2 in central
:: resolution report :: resolve 803ms :: artifacts dl 81ms
	:: modules in use:
	com.github.luben#zstd-jni;1.4.8-1 from central in [default]
	org.apache.commons#commons-pool2;2.6.2 from central in [default]
	org.apache.kafka#kafka-clients;2.6.0 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]
	org.lz4#lz4-java;1.7.1 from central in [default]
	org.slf4j#slf4j-api;1.7.30 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.8.2 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-44fb42eb-2eac-450f-810a-ab97739a323f
	confs: [default]
	0 artifacts copied, 9 already retrieved (0kB/49ms)
21/07/14 10:53:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/07/14 10:53:49 INFO SparkContext: Running Spark version 3.1.2
21/07/14 10:53:49 INFO ResourceUtils: ==============================================================
21/07/14 10:53:49 INFO ResourceUtils: No custom resources configured for spark.driver.
21/07/14 10:53:49 INFO ResourceUtils: ==============================================================
21/07/14 10:53:49 INFO SparkContext: Submitted application: STEDI-joining
21/07/14 10:53:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/07/14 10:53:49 INFO ResourceProfile: Limiting resource is cpu
21/07/14 10:53:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/07/14 10:53:49 INFO SecurityManager: Changing view acls to: spark
21/07/14 10:53:49 INFO SecurityManager: Changing modify acls to: spark
21/07/14 10:53:49 INFO SecurityManager: Changing view acls groups to: 
21/07/14 10:53:49 INFO SecurityManager: Changing modify acls groups to: 
21/07/14 10:53:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
21/07/14 10:53:49 INFO Utils: Successfully started service 'sparkDriver' on port 34579.
21/07/14 10:53:49 INFO SparkEnv: Registering MapOutputTracker
21/07/14 10:53:50 INFO SparkEnv: Registering BlockManagerMaster
21/07/14 10:53:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/14 10:53:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/14 10:53:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/14 10:53:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f8af4355-7bc1-4582-8364-aba1844f3192
21/07/14 10:53:50 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
21/07/14 10:53:50 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/14 10:53:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/07/14 10:53:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://1127a61f386e:4040
21/07/14 10:53:51 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar at spark://1127a61f386e:34579/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar at spark://1127a61f386e:34579/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar at spark://1127a61f386e:34579/jars/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at spark://1127a61f386e:34579/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://1127a61f386e:34579/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar at spark://1127a61f386e:34579/jars/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at spark://1127a61f386e:34579/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar at spark://1127a61f386e:34579/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at spark://1127a61f386e:34579/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar
21/07/14 10:53:51 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar
21/07/14 10:53:51 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.kafka_kafka-clients-2.6.0.jar
21/07/14 10:53:51 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.commons_commons-pool2-2.6.2.jar
21/07/14 10:53:51 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.spark-project.spark_unused-1.0.0.jar
21/07/14 10:53:51 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar at file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/com.github.luben_zstd-jni-1.4.8-1.jar
21/07/14 10:53:51 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.lz4_lz4-java-1.7.1.jar
21/07/14 10:53:51 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.xerial.snappy_snappy-java-1.1.8.2.jar
21/07/14 10:53:51 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.slf4j_slf4j-api-1.7.30.jar
21/07/14 10:53:51 INFO Executor: Starting executor ID driver on host 1127a61f386e
21/07/14 10:53:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar
21/07/14 10:53:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.xerial.snappy_snappy-java-1.1.8.2.jar
21/07/14 10:53:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar
21/07/14 10:53:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/com.github.luben_zstd-jni-1.4.8-1.jar
21/07/14 10:53:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.commons_commons-pool2-2.6.2.jar
21/07/14 10:53:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.spark-project.spark_unused-1.0.0.jar
21/07/14 10:53:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.lz4_lz4-java-1.7.1.jar
21/07/14 10:53:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.kafka_kafka-clients-2.6.0.jar
21/07/14 10:53:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1626260029258
21/07/14 10:53:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.slf4j_slf4j-api-1.7.30.jar
21/07/14 10:53:51 INFO Executor: Fetching spark://1127a61f386e:34579/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1626260029258
21/07/14 10:53:52 INFO TransportClientFactory: Successfully created connection to 1127a61f386e/172.27.0.3:34579 after 79 ms (0 ms spent in bootstraps)
21/07/14 10:53:52 INFO Utils: Fetching spark://1127a61f386e:34579/jars/org.lz4_lz4-java-1.7.1.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp3068400522600987401.tmp
21/07/14 10:53:52 INFO Utils: /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp3068400522600987401.tmp has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.lz4_lz4-java-1.7.1.jar
21/07/14 10:53:52 INFO Executor: Adding file:/tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.lz4_lz4-java-1.7.1.jar to class loader
21/07/14 10:53:52 INFO Executor: Fetching spark://1127a61f386e:34579/jars/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1626260029258
21/07/14 10:53:52 INFO Utils: Fetching spark://1127a61f386e:34579/jars/com.github.luben_zstd-jni-1.4.8-1.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp6969482930982141523.tmp
21/07/14 10:53:52 INFO Utils: /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp6969482930982141523.tmp has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/com.github.luben_zstd-jni-1.4.8-1.jar
21/07/14 10:53:52 INFO Executor: Adding file:/tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/com.github.luben_zstd-jni-1.4.8-1.jar to class loader
21/07/14 10:53:52 INFO Executor: Fetching spark://1127a61f386e:34579/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar with timestamp 1626260029258
21/07/14 10:53:52 INFO Utils: Fetching spark://1127a61f386e:34579/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp8576942435047640063.tmp
21/07/14 10:53:52 INFO Utils: /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp8576942435047640063.tmp has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar
21/07/14 10:53:52 INFO Executor: Adding file:/tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar to class loader
21/07/14 10:53:52 INFO Executor: Fetching spark://1127a61f386e:34579/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1626260029258
21/07/14 10:53:52 INFO Utils: Fetching spark://1127a61f386e:34579/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp8875014407465975466.tmp
21/07/14 10:53:52 INFO Utils: /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp8875014407465975466.tmp has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.commons_commons-pool2-2.6.2.jar
21/07/14 10:53:52 INFO Executor: Adding file:/tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.commons_commons-pool2-2.6.2.jar to class loader
21/07/14 10:53:52 INFO Executor: Fetching spark://1127a61f386e:34579/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1626260029258
21/07/14 10:53:52 INFO Utils: Fetching spark://1127a61f386e:34579/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp7711084649720068503.tmp
21/07/14 10:53:52 INFO Utils: /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp7711084649720068503.tmp has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.xerial.snappy_snappy-java-1.1.8.2.jar
21/07/14 10:53:52 INFO Executor: Adding file:/tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.xerial.snappy_snappy-java-1.1.8.2.jar to class loader
21/07/14 10:53:52 INFO Executor: Fetching spark://1127a61f386e:34579/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1626260029258
21/07/14 10:53:52 INFO Utils: Fetching spark://1127a61f386e:34579/jars/org.slf4j_slf4j-api-1.7.30.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp7396379031062777222.tmp
21/07/14 10:53:52 INFO Utils: /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp7396379031062777222.tmp has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.slf4j_slf4j-api-1.7.30.jar
21/07/14 10:53:52 INFO Executor: Adding file:/tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.slf4j_slf4j-api-1.7.30.jar to class loader
21/07/14 10:53:52 INFO Executor: Fetching spark://1127a61f386e:34579/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar with timestamp 1626260029258
21/07/14 10:53:52 INFO Utils: Fetching spark://1127a61f386e:34579/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp8537934079406961031.tmp
21/07/14 10:53:52 INFO Utils: /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp8537934079406961031.tmp has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar
21/07/14 10:53:52 INFO Executor: Adding file:/tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar to class loader
21/07/14 10:53:52 INFO Executor: Fetching spark://1127a61f386e:34579/jars/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1626260029258
21/07/14 10:53:52 INFO Utils: Fetching spark://1127a61f386e:34579/jars/org.apache.kafka_kafka-clients-2.6.0.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp5372215026631915266.tmp
21/07/14 10:53:52 INFO Utils: /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp5372215026631915266.tmp has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.kafka_kafka-clients-2.6.0.jar
21/07/14 10:53:52 INFO Executor: Adding file:/tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.apache.kafka_kafka-clients-2.6.0.jar to class loader
21/07/14 10:53:52 INFO Executor: Fetching spark://1127a61f386e:34579/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1626260029258
21/07/14 10:53:52 INFO Utils: Fetching spark://1127a61f386e:34579/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp3562646349585494063.tmp
21/07/14 10:53:52 INFO Utils: /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/fetchFileTemp3562646349585494063.tmp has been previously copied to /tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.spark-project.spark_unused-1.0.0.jar
21/07/14 10:53:52 INFO Executor: Adding file:/tmp/spark-1ee5b17c-2a24-4098-8e15-8a2d0038ee0d/userFiles-f31865ba-a566-4b1e-94aa-d84548ecf733/org.spark-project.spark_unused-1.0.0.jar to class loader
21/07/14 10:53:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42113.
21/07/14 10:53:52 INFO NettyBlockTransferService: Server created on 1127a61f386e:42113
21/07/14 10:53:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/14 10:53:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1127a61f386e, 42113, None)
21/07/14 10:53:52 INFO BlockManagerMasterEndpoint: Registering block manager 1127a61f386e:42113 with 366.3 MiB RAM, BlockManagerId(driver, 1127a61f386e, 42113, None)
21/07/14 10:53:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1127a61f386e, 42113, None)
21/07/14 10:53:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1127a61f386e, 42113, None)
21/07/14 10:53:53 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/bitnami/spark/spark-warehouse').
21/07/14 10:53:53 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
^CTraceback (most recent call last):
  File "/home/workspace/project/starter/sparkpykafkajoin.py", line 158, in <module>
    .option('checkpointLocation', '/tmp/kafkacheckpoint') \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py", line 101, in awaitTermination
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1303, in __call__
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1033, in send_command
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1200, in send_command
  File "/opt/bitnami/python/lib/python3.6/socket.py", line 586, in readinto
    return self._sock.recv_into(b)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 285, in signal_handler
KeyboardInterrupt
21/07/14 10:54:53 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@75911e0c is aborting.
21/07/14 10:54:53 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@75911e0c aborted.
21/07/14 10:54:53 WARN Shell: Interrupted while joining on: Thread[Thread-17151,5,]
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)
	at org.apache.hadoop.util.Shell.run(Shell.java:901)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)
	at org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:208)
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1000)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:989)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:960)
	at org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:131)
	at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:726)
	at org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:251)
	at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:699)
	at org.apache.hadoop.fs.FileContext.rename(FileContext.java:1032)
	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:335)
	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:147)
	at net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:159)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:152)
	at org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:351)
	at org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:296)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:617)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:415)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:597)
	at org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:111)
	at org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:111)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:414)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:431)
	at org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/07/14 10:54:53 WARN Shell: Interrupted while joining on: Thread[Thread-17161,5,main]
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)
	at org.apache.hadoop.util.Shell.run(Shell.java:901)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)
	at org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:208)
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1000)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:989)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:960)
	at org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:131)
	at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:726)
	at org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:251)
	at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:699)
	at org.apache.hadoop.fs.FileContext.rename(FileContext.java:1032)
	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:335)
	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:147)
	at net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:159)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:152)
	at org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:351)
	at org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:296)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:617)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:415)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:597)
	at org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:111)
	at org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:111)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:414)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:431)
	at org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/07/14 10:54:53 ERROR MicroBatchExecution: Query [id = b93f1f08-2db0-4db3-b96d-553beec707d4, runId = f942b954-7c1c-4f96-bcde-b9367dd62e7e] terminated with error
org.apache.spark.SparkException: Writing job aborted.
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:388)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:297)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:304)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)
	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2965)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2965)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:589)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:584)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:584)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:194)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:188)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:334)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
Caused by: org.apache.spark.SparkException: Job 3 cancelled as part of cancellation of all jobs
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2154)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:972)
	at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:971)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2410)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)
	... 40 more
21/07/14 10:54:53 WARN Shell: Interrupted while joining on: Thread[Thread-17158,5,main]
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)
	at org.apache.hadoop.util.Shell.run(Shell.java:901)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)
	at org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:208)
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1000)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:989)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:960)
	at org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:131)
	at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:726)
	at org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:251)
	at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:699)
	at org.apache.hadoop.fs.FileContext.rename(FileContext.java:1032)
	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:335)
	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:147)
	at net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:159)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:152)
	at org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:351)
	at org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:297)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:617)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:416)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:597)
	at org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:111)
	at org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:111)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:414)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:431)
	at org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/07/14 10:54:53 ERROR Utils: Aborting task
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1039)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:248)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)
	at org.apache.spark.scheduler.OutputCommitCoordinator.canCommit(OutputCommitCoordinator.scala:104)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:421)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/07/14 10:54:53 ERROR DataWritingSparkTask: Aborting commit for partition 65 (task 673, attempt 0, stage 11.0)
21/07/14 10:54:53 ERROR DataWritingSparkTask: Aborted commit for partition 65 (task 673, attempt 0, stage 11.0)
21/07/14 10:54:53 WARN Shell: Interrupted while joining on: Thread[Thread-17166,5,main]
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)
	at org.apache.hadoop.util.Shell.run(Shell.java:901)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)
	at org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:208)
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1000)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:989)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:960)
	at org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1513)
	at org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:204)
	at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:769)
	at org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:251)
	at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:699)
	at org.apache.hadoop.fs.FileContext.rename(FileContext.java:1032)
	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:335)
	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:147)
	at net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:159)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:152)
	at org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:351)
	at org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:297)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:617)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:416)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:597)
	at org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:111)
	at org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:111)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:414)
	at org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:431)
	at org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/07/14 10:54:53 ERROR Utils: Aborting task
org.apache.spark.executor.CommitDeniedException: Commit denied for partition 64 (task 672, attempt 0, stage 11.0)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:431)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/07/14 10:54:53 ERROR DataWritingSparkTask: Aborting commit for partition 64 (task 672, attempt 0, stage 11.0)
21/07/14 10:54:53 ERROR DataWritingSparkTask: Aborted commit for partition 64 (task 672, attempt 0, stage 11.0)
21/07/14 10:54:53 ERROR Utils: Aborting task
org.apache.spark.executor.CommitDeniedException: Commit denied for partition 66 (task 674, attempt 0, stage 11.0)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:431)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/07/14 10:54:53 ERROR DataWritingSparkTask: Aborting commit for partition 66 (task 674, attempt 0, stage 11.0)
21/07/14 10:54:53 ERROR DataWritingSparkTask: Aborted commit for partition 66 (task 674, attempt 0, stage 11.0)
21/07/14 10:54:53 WARN TaskSetManager: Lost task 64.0 in stage 11.0 (TID 672) (1127a61f386e executor driver): TaskKilled (Stage cancelled)
21/07/14 10:54:53 WARN TaskSetManager: Lost task 65.0 in stage 11.0 (TID 673) (1127a61f386e executor driver): TaskKilled (Stage cancelled)
21/07/14 10:54:53 WARN TaskSetManager: Lost task 66.0 in stage 11.0 (TID 674) (1127a61f386e executor driver): TaskKilled (Stage cancelled)
21/07/14 10:54:53 ERROR Utils: Aborting task
org.apache.spark.executor.CommitDeniedException: Commit denied for partition 67 (task 675, attempt 0, stage 11.0)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:431)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/07/14 10:54:53 ERROR DataWritingSparkTask: Aborting commit for partition 67 (task 675, attempt 0, stage 11.0)
21/07/14 10:54:53 ERROR DataWritingSparkTask: Aborted commit for partition 67 (task 675, attempt 0, stage 11.0)
21/07/14 10:54:53 ERROR TaskSchedulerImpl: Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$3719/171769813@6bab8e65 rejected from java.util.concurrent.ThreadPoolExecutor@70d222b9[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 675]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:771)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
